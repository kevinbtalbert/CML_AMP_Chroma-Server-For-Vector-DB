{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b331a3ae-f63b-47c1-bfae-6e1efc5062b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Initialize a connection to the running Chroma DB server\n",
    "import chromadb\n",
    "import os\n",
    "\n",
    "## Use the following line to connect from within CML\n",
    "chroma_client = chromadb.Client() \n",
    "\n",
    "## Use the following line to connect from outside CML \n",
    "## USER TO DO: Set host with the URL to the application where the server is hosted CML--> Applications\n",
    "## USER TO DO: Enable unauthenticated app access for the Application\n",
    "HOST='https://cml-chroma-server-ysovdh.ml-b74f8940-b97.go01-dem.ylcu-atmi.cloudera.site/'\n",
    "chroma_client = chromadb.HttpClient(host=HOST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c4b448f4-877d-4854-9c17-9247441b5b4f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialising Chroma DB connection...\n",
      "Getting 'cml-default' as object...\n",
      "Success\n",
      "Total number of embeddings in Chroma DB index is 14\n"
     ]
    }
   ],
   "source": [
    "from chromadb.utils import embedding_functions\n",
    "\n",
    "EMBEDDING_MODEL_REPO = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "EMBEDDING_MODEL_NAME = \"all-mpnet-base-v2\"\n",
    "EMBEDDING_FUNCTION = embedding_functions.SentenceTransformerEmbeddingFunction(model_name=EMBEDDING_MODEL_NAME)\n",
    "\n",
    "COLLECTION_NAME = os.getenv('COLLECTION_NAME')\n",
    "\n",
    "print(\"initialising Chroma DB connection...\")\n",
    "\n",
    "print(f\"Getting '{COLLECTION_NAME}' as object...\")\n",
    "try:\n",
    "    chroma_client.get_collection(name=COLLECTION_NAME, embedding_function=EMBEDDING_FUNCTION)\n",
    "    print(\"Success\")\n",
    "    collection = chroma_client.get_collection(name=COLLECTION_NAME, embedding_function=EMBEDDING_FUNCTION)\n",
    "except:\n",
    "    print(\"Creating new collection...\")\n",
    "    collection = chroma_client.create_collection(name=COLLECTION_NAME, embedding_function=EMBEDDING_FUNCTION)\n",
    "    print(\"Success\")\n",
    "\n",
    "# Get latest statistics from index\n",
    "current_collection_stats = collection.count()\n",
    "print('Total number of embeddings in Chroma DB index is ' + str(current_collection_stats))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9e0835b3-87e3-4d67-abe2-6c6538f8f488",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Sample add to Chroma vector DB\n",
    "file_path = '/example/of/file/path/to/doc.txt'\n",
    "classification = \"public\"\n",
    "text = \"This is a sample document which would represent content for a semantic search.\"\n",
    "\n",
    "collection.add(\n",
    "    documents=[text],\n",
    "    metadatas=[{\"classification\": classification}],\n",
    "    ids=[file_path]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a5448a26",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ids': [['/home/cdsw/3_job-populate-sample-vectors/sample-data/iceberg/iceberg-snippet.txt', '/home/cdsw/3_job-populate-sample-vectors/sample-data/ozone/ozone-snippet.txt']], 'distances': [[1.1674057912671203, 1.400256050073526]], 'embeddings': None, 'metadatas': [[{'classification': 'public'}, {'classification': 'public'}]], 'documents': [[\"Apache Iceberg is an open table format for huge analytic datasets. Iceberg adds tables to compute engines including Spark, Trino, PrestoDB, Flink, Hive and Impala using a high-performance table format that works just like a SQL table.\\n\\nUser experience\\nIceberg avoids unpleasant surprises. Schema evolution works and won't inadvertently un-delete data. Users don't need to know about partitioning to get fast queries.\\n\\nSchema evolution supports add, drop, update, or rename, and has no side-effects\\nHidden partitioning prevents user mistakes that cause silently incorrect results or extremely slow queries\\nPartition layout evolution can update the layout of a table as data volume or query patterns change\\nTime travel enables reproducible queries that use exactly the same table snapshot, or lets users easily examine changes\\nVersion rollback allows users to quickly correct problems by resetting tables to a good state\\n\", 'Apache Ozone is a scalable, redundant, and distributed object store optimized for big data workloads. Apart from scaling to billions of objects of varying sizes, applications that use frameworks like Apache Spark, Apache YARN and Apache Hive work natively on Ozone object store without any modifications.\\n\\nHDFS works best when most of the files are large but HDFS suffers from small file limitations. Ozone is a distributed key-value object store that can manage both small and large files alike. Ozone natively supports the S3 API and provides a Hadoop-compatible file system interface. Ozone object store is available in a CDP Private Cloud Base deployment.\\n']]}\n"
     ]
    }
   ],
   "source": [
    "## Query Chroma vector DB \n",
    "## This query returns the two most similar results from a semantic search\n",
    "results = collection.query(\n",
    "    query_texts=[\"What is Apache Iceberg?\"],\n",
    "    n_results=2\n",
    "    # where={\"metadata_field\": \"is_equal_to_this\"}, # optional filter\n",
    "    # where_document={\"$contains\":\"search_string\"}  # optional filter\n",
    ")\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c30e5a6a-265b-4fa6-ae37-f5f38fe6296c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------- RESULT 1 ----------------\n",
      "\n",
      "FILE PATH: /home/cdsw/3_job-populate-sample-vectors/sample-data/iceberg/iceberg-snippet.txt\n",
      "CLASSIFICATION: public\n",
      "DOCUMENT: Apache Iceberg is an open table format for huge analytic datasets. Iceberg adds tables to compute engines including Spark, Trino, PrestoDB, Flink, Hive and Impala using a high-performance table format that works just like a SQL table.\n",
      "\n",
      "User experience\n",
      "Iceberg avoids unpleasant surprises. Schema evolution works and won't inadvertently un-delete data. Users don't need to know about partitioning to get fast queries.\n",
      "\n",
      "Schema evolution supports add, drop, update, or rename, and has no side-effects\n",
      "Hidden partitioning prevents user mistakes that cause silently incorrect results or extremely slow queries\n",
      "Partition layout evolution can update the layout of a table as data volume or query patterns change\n",
      "Time travel enables reproducible queries that use exactly the same table snapshot, or lets users easily examine changes\n",
      "Version rollback allows users to quickly correct problems by resetting tables to a good state\n",
      "\n",
      "\n",
      "FULL DOCUMENT (FROM FILE): Apache Iceberg is an open table format for huge analytic datasets. Iceberg adds tables to compute engines including Spark, Trino, PrestoDB, Flink, Hive and Impala using a high-performance table format that works just like a SQL table.\n",
      "\n",
      "User experience\n",
      "Iceberg avoids unpleasant surprises. Schema evolution works and won't inadvertently un-delete data. Users don't need to know about partitioning to get fast queries.\n",
      "\n",
      "Schema evolution supports add, drop, update, or rename, and has no side-effects\n",
      "Hidden partitioning prevents user mistakes that cause silently incorrect results or extremely slow queries\n",
      "Partition layout evolution can update the layout of a table as data volume or query patterns change\n",
      "Time travel enables reproducible queries that use exactly the same table snapshot, or lets users easily examine changes\n",
      "Version rollback allows users to quickly correct problems by resetting tables to a good state\n",
      "\n",
      "\n",
      "------------- RESULT 2 ----------------\n",
      "\n",
      "FILE PATH: /home/cdsw/3_job-populate-sample-vectors/sample-data/ozone/ozone-snippet.txt\n",
      "CLASSIFICATION: public\n",
      "DOCUMENT: Apache Ozone is a scalable, redundant, and distributed object store optimized for big data workloads. Apart from scaling to billions of objects of varying sizes, applications that use frameworks like Apache Spark, Apache YARN and Apache Hive work natively on Ozone object store without any modifications.\n",
      "\n",
      "HDFS works best when most of the files are large but HDFS suffers from small file limitations. Ozone is a distributed key-value object store that can manage both small and large files alike. Ozone natively supports the S3 API and provides a Hadoop-compatible file system interface. Ozone object store is available in a CDP Private Cloud Base deployment.\n",
      "\n",
      "\n",
      "FULL DOCUMENT (FROM FILE): Apache Ozone is a scalable, redundant, and distributed object store optimized for big data workloads. Apart from scaling to billions of objects of varying sizes, applications that use frameworks like Apache Spark, Apache YARN and Apache Hive work natively on Ozone object store without any modifications.\n",
      "\n",
      "HDFS works best when most of the files are large but HDFS suffers from small file limitations. Ozone is a distributed key-value object store that can manage both small and large files alike. Ozone natively supports the S3 API and provides a Hadoop-compatible file system interface. Ozone object store is available in a CDP Private Cloud Base deployment.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Helper function to return the Knowledge Base doc based on Knowledge Base ID (relative file path)\n",
    "def load_context_chunk_from_data(id_path):\n",
    "    with open(id_path, \"r\") as f: # Open file in read mode\n",
    "        return f.read()\n",
    "    \n",
    "## Clean up output and display full file\n",
    "for i in range(len(results['ids'][0])):\n",
    "    file_path = results['ids'][0][i]\n",
    "    classification = results['metadatas'][0][i]['classification']\n",
    "    document = results['documents'][0][i]\n",
    "    \n",
    "    print(\"------------- RESULT \" + str(i+1) + \" ----------------\\n\")\n",
    "    print(f\"FILE PATH: {file_path}\")\n",
    "    print(f\"CLASSIFICATION: {classification}\")\n",
    "    print(f\"DOCUMENT: {document}\\n\")\n",
    "    print(f\"FULL DOCUMENT (FROM FILE): {load_context_chunk_from_data(file_path)}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120d6782-c253-4d0c-84bf-a4628cb4a0ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
